import torch
import time

# Thiáº¿t láº­p GPU náº¿u cÃ³
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Äá»‹nh nghÄ©a thÃ´ng sá»‘
B, H, N, M, D = 2, 4, 8, 8, 64  # Batch size, Heads, Seq len, Seq len, Dim
Q = torch.randn(B, H, N, D, device=device)
K = torch.randn(B, H, M, D, device=device)
V = torch.randn(B, H, M, D, device=device)
W_out = torch.randn(D, D, device=device)
d = D ** 0.5

# ğŸ”¹ HÃ m gá»™p
def attention_fused(Q, K, V, W_out, d):
    return torch.einsum(
        'bhnm,bhmd,dd->bhnd',
        torch.softmax(torch.einsum('bhnd,bhmd->bhnm', Q, K) / d, dim=-1),
        V,
        W_out
    )
    

# ğŸ”¹ HÃ m tÃ¡ch láº»
def attention_separate(Q, K, V, W_out, d):
    A = torch.einsum('bhnd,bhmd->bhnm', Q, K) / d  # Attention scores
    A = torch.softmax(A, dim=-1)  # Apply softmax
    O = torch.einsum('bhnm,bhmd->bhnd', A, V)  # Weighted sum over V
    O = torch.einsum('bhnd,dd->bhnd', O, W_out)  # Apply output weights
    return O

# ğŸ”¹ Äo thá»i gian thá»±c thi trÃªn GPU
def benchmark(func, *args, runs=100):
    torch.cuda.synchronize()
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    
    start.record()
    for _ in range(runs):
        _ = func(*args)
    end.record()
    
    torch.cuda.synchronize()
    return start.elapsed_time(end) / runs  # Thá»i gian trung bÃ¬nh má»—i láº§n cháº¡y (ms)

# Cháº¡y benchmark
runs = 1000  # Sá»‘ láº§n cháº¡y Ä‘á»ƒ Ä‘o tá»‘c Ä‘á»™ trung bÃ¬nh
# time_fused = benchmark(attention_fused, Q, K, V, W_out, d, runs=runs)
# # ğŸ”¹ In káº¿t quáº£
# print(f"HÃ m gá»™p: {time_fused:.4f} ms")

time_separate = benchmark(attention_separate, Q, K, V, W_out, d, runs=runs)
print(f"HÃ m tÃ¡ch láº»: {time_separate:.4f} ms")
# print(f"TÄƒng tá»‘c: {100 * (time_separate - time_fused) / time_separate:.2f}%")
